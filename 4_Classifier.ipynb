{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6035d80-b70b-4411-becb-2689375b385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./2_Methods.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e06b5e92-ec11-4e85-a74f-e507cb38ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_tanh(x, beta=1.0):\n",
    "    return (x * K.tanh(beta * x) + 1) / 2 # Alternative to Sigmoid? #not yet tested\n",
    "\n",
    "# Custom Binary Crossentropy Loss function which penalizes false positives more than false negatives:\n",
    "class WeightedBinaryCrossentropy(tf.keras.losses.Loss):\n",
    "    def __init__(self, false_positive_penalty=0.7, false_negative_penalty=0.3):\n",
    "        super().__init__()\n",
    "        self.false_positive_penalty = false_positive_penalty        \n",
    "        self.false_negative_penalty = false_negative_penalty\n",
    "\n",
    "    def call(self, y_true, y_pred):        \n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32)\n",
    "        \n",
    "        bce_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred, from_logits=False)\n",
    "\n",
    "        false_positive = tf.reduce_sum((1 - y_true) * tf.round(y_pred))\n",
    "        false_negative = tf.reduce_sum(y_true * (1 - tf.round(y_pred)))\n",
    "\n",
    "        fp_penalty = self.false_positive_penalty * false_positive\n",
    "        fn_penalty = self.false_negative_penalty * false_negative\n",
    "\n",
    "        loss = bce_loss + fp_penalty + fn_penalty\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd37060a-1771-4ecb-82cb-f67e7f07e6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:      \n",
    "    DEFAULT_EPOCHS=350\n",
    "    \n",
    "    def __init__(self, autoencoder, optimizer_name, learning_rate, loss_function_name,\n",
    "                 metrics=DEFAULT_METRICS, test_train_val_ratios=(0.8, 0.1, 0.1), \n",
    "                 random_state = None, epochs=DEFAULT_EPOCHS, early_stopping_patience=None,\n",
    "                 cross_validation_split=None, verbose=None, init=True):        \n",
    "        if len(test_train_val_ratios) != 3 or sum(test_train_val_ratios) != 1:\n",
    "            raise ValueError(f'The test, train and validation ratio must be a tule of three numbers whose sum equals 1.0! (Specified: {test_train_val_ratios})')\n",
    "\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.autoencoder = autoencoder\n",
    "        self.feature_count = autoencoder.input_data.shape[1]\n",
    "\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_function_name = loss_function_name\n",
    "        self.metrics = metrics\n",
    "        \n",
    "        self.max_epochs = epochs\n",
    "        self.test_train_val_ratios = test_train_val_ratios\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.cross_validation_split = cross_validation_split\n",
    "\n",
    "        self.model = None\n",
    "        self.fit_history = None\n",
    "        self.train_evaluation = None\n",
    "        self.test_evaluation = None\n",
    "        self.positive_sample_size = 1_000        \n",
    "        self.negative_sample_size = 20_000\n",
    "        self.epochs = None\n",
    "\n",
    "        self.kfold = None\n",
    "        self.kfold_evaluations = None\n",
    "        \n",
    "        self.__reset_state()\n",
    "\n",
    "        self.initialized = False\n",
    "\n",
    "        if init:\n",
    "            self.initialize()\n",
    "            \n",
    "    def __reset_state(self):\n",
    "        if self.random_state == None: \n",
    "            self.random_state = random.randrange(1000000000)\n",
    "        reset_random_state(self.random_state)\n",
    "\n",
    "    def __create_model(self):\n",
    "        return tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(self.feature_count, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(self.feature_count/2, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(self.feature_count/4, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "        ])\n",
    "\n",
    "    def __create_model_experimental(self):\n",
    "        return tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(self.feature_count, activation=tf.keras.layers.LeakyReLU()), #try leakyrelu\n",
    "            tf.keras.layers.Dense(self.feature_count/2, activation=tf.keras.layers.LeakyReLU()),\n",
    "            tf.keras.layers.Dense(self.feature_count/4, activation=tf.keras.layers.LeakyReLU()),\n",
    "            tf.keras.layers.Dense(1, activation=normalized_tanh) #try tanh\n",
    "        ])\n",
    "\n",
    "    def __train(self, data, epochs=DEFAULT_EPOCHS, verbose=False):        \n",
    "        X_train, X_test_val, y_train, y_test_val = data\n",
    "        X_test, X_val, y_test, y_val = self.__split_test_val_data(X_test_val, y_test_val)\n",
    "        \n",
    "        callbacks = []\n",
    "        if self.early_stopping_patience != None:            \n",
    "            callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=self.early_stopping_patience, restore_best_weights=True))\n",
    "\n",
    "        if self.cross_validation_split == 1:           \n",
    "            X_train = np.concatenate((X_train, X_test), axis=0)\n",
    "            y_train = np.concatenate((y_train, y_test), axis=0)\n",
    "            \n",
    "        self.fit_history = self.model.fit(X_train, y_train, epochs=epochs, \n",
    "                                          validation_data=(X_val, y_val),\n",
    "                                          callbacks=callbacks, \n",
    "                                          verbose=verbose)     \n",
    "        self.epochs = len(self.fit_history.epoch)\n",
    "\n",
    "        train_evaluation = self.model.evaluate(X_train, y_train, return_dict=True, verbose=None)\n",
    "        test_evaluation = self.model.evaluate(X_test, y_test, return_dict=True, verbose=None) if self.cross_validation_split != 1 else None\n",
    "\n",
    "        return (train_evaluation, test_evaluation)\n",
    "\n",
    "    def __split_dataset(self, X, y):\n",
    "        return train_test_split(X, y, test_size=self.test_train_val_ratios[1], shuffle=True, random_state=random_state) # returns (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    def __split_test_val_data(self, X, y):\n",
    "        test_split = self.test_train_val_ratios[1]\n",
    "        val_split = self.test_train_val_ratios[2]\n",
    "        val_split = val_split / (test_split + val_split)\n",
    "\n",
    "        return train_test_split(X, y, test_size=val_split, shuffle=True, random_state=random_state) # returns (X_test, X_val, y_test, y_val)\n",
    "\n",
    "    def __compile_output(self):\n",
    "        meta_info = {\n",
    "            'optimizer': self.optimizer_name,\n",
    "            'learning rate': self.learning_rate,\n",
    "            'loss function': self.loss_function_name,\n",
    "            'random seed': self.random_state,\n",
    "            'epochs (max)': self.max_epochs,\n",
    "            'early stopping': self.early_stopping_patience,\n",
    "            'epochs trained': self.epochs,\n",
    "            'early stopping patience': self.early_stopping_patience,\n",
    "        }\n",
    "        \n",
    "        if self.cross_validation_split == None or self.cross_validation_split == 1:\n",
    "            self.output = {\n",
    "                    'meta-info': meta_info,\n",
    "                    'testing': self.train_evaluation,\n",
    "                    'training': self.test_evaluation,\n",
    "                    'history': self.fit_history,\n",
    "                }\n",
    "        else: \n",
    "            meta_info['k-fold split'] = self.cross_validation_split\n",
    "            self.output = {\n",
    "                'meta-info': meta_info,\n",
    "                'k-fold means testing': self.kfold_train_evaluation, #sum average over all train evals in kfold_evaluations\n",
    "                'k-fold means training': self.kfold_test_evaluation, #sum average over all test evals in kfold_evaluations\n",
    "                'k-fold histories': self.kfold_histories,\n",
    "                'k-fold evaluations': self.kfold_evaluations\n",
    "            }\n",
    "\n",
    "    def __get_loss_function(self, loss_function_name):\n",
    "        if loss_function_name == 'WeightedBinaryCrossentropy':\n",
    "            return WeightedBinaryCrossentropy()\n",
    "        else:\n",
    "            return tf.keras.losses.get(loss_function_name)\n",
    "\n",
    "    def predict(self, X, verbose=None):\n",
    "        return self.model.predict(self.autoencoder.predict(X, verbose=None), verbose=verbose)\n",
    "            \n",
    "    def initialize(self):\n",
    "        if self.initialized:\n",
    "            raise ValueError(f'Cannot initialize already initialized autoencoder!')\n",
    "            \n",
    "        y_pos_value = positive_class\n",
    "        y_neg_value = negative_class\n",
    "        if self.loss_function_name == 'CosineSimilarity':\n",
    "            y_pos_value = float(y_pos_value)\n",
    "            y_neg_value = float(y_neg_value)\n",
    "\n",
    "        positive_X = self.autoencoder.generate_positive(self.positive_sample_size)\n",
    "        negative_X = self.autoencoder.generate_negative(self.negative_sample_size)\n",
    "        \n",
    "        self.X = np.concatenate([positive_X, negative_X])\n",
    "        self.y = np.concatenate([[y_pos_value for i in range(positive_X.shape[0])], \n",
    "                                 [y_neg_value for i in range(negative_X.shape[0])]])\n",
    "\n",
    "        if self.cross_validation_split == None or self.cross_validation_split == 1:\n",
    "            self.model = self.__create_model()\n",
    "            self.optimizer = globals()[self.optimizer_name](self.learning_rate)\n",
    "            self.loss_function = self.__get_loss_function(self.loss_function_name)\n",
    "                \n",
    "            self.model.compile(\n",
    "                optimizer=self.optimizer,\n",
    "                loss=self.loss_function,\n",
    "                metrics=self.metrics\n",
    "            )\n",
    "\n",
    "            (self.train_evaluation, self.test_evaluation) = self.__train(self.__split_dataset(self.X, self.y), epochs=self.max_epochs, verbose=self.verbose)\n",
    "\n",
    "        else:\n",
    "            best_loss = None\n",
    "            best_model = None\n",
    "            \n",
    "            self.kfold = KFold(n_splits=self.cross_validation_split, shuffle=True, random_state=self.random_state)\n",
    "            self.kfold_evaluations = []\n",
    "            self.kfold_histories = []\n",
    "            kfold_test_evaluation = []\n",
    "            kfold_train_evaluation = []\n",
    "\n",
    "            for train_idx, test_idx in self.kfold.split(self.X):\n",
    "                X_train, X_test = self.X[train_idx], self.X[test_idx]\n",
    "                y_train, y_test = self.y[train_idx], self.y[test_idx]\n",
    "\n",
    "                self.model = self.__create_model()\n",
    "                optimizer = globals()[self.optimizer_name](self.learning_rate)\n",
    "                loss_function = self.__get_loss_function(self.loss_function_name)\n",
    "                self.model.compile(\n",
    "                    optimizer=optimizer,\n",
    "                    loss=loss_function,\n",
    "                    metrics=self.metrics\n",
    "                )\n",
    "            \n",
    "                (train_evaluation, test_evaluation) = self.__train((X_train, X_test, y_train, y_test), epochs=self.max_epochs, verbose=self.verbose)\n",
    "                \n",
    "                kfold_train_evaluation.append(train_evaluation)\n",
    "                kfold_test_evaluation.append(test_evaluation)\n",
    "                self.kfold_evaluations.append({\n",
    "                    'training': train_evaluation,\n",
    "                    'testing': test_evaluation\n",
    "                })\n",
    "                self.kfold_histories.append(\n",
    "                    self.fit_history\n",
    "                )\n",
    "                self.kfold_train_evaluation = dict_mean(kfold_train_evaluation)\n",
    "                self.kfold_test_evaluation = dict_mean(kfold_test_evaluation)\n",
    "\n",
    "                test_loss = test_evaluation[\"loss\"]\n",
    "\n",
    "                if best_loss == None or test_loss < best_loss:\n",
    "                    best_loss = test_loss\n",
    "                    best_model = self.model\n",
    "\n",
    "            self.fit_history = None\n",
    "            self.train_evaluation = None\n",
    "            self.test_evaluation = None\n",
    "            self.model = best_model\n",
    "\n",
    "        self.__compile_output()\n",
    "        self.initialized = True\n",
    "\n",
    "    def print_evaluation(self, show_history=False):\n",
    "        print(f'Classifier Model:')\n",
    "        print(f'  autoencoder: {self.autoencoder.optimizer_name} (lr={self.autoencoder.learning_rate}) - {self.autoencoder.loss_function_name} - seed={self.autoencoder.random_state}')\n",
    "        print(f'    euclidean dist (augmented): {self.autoencoder.euclidean_dist_positive_class}')\n",
    "        print(f'    euclidean dist delta: {self.autoencoder.euclidean_dist_delta}')\n",
    "        print(f'    euclidean dist ratio: {self.autoencoder.euclidean_dist_ratio}')\n",
    "        print(f'  optimizer: {self.optimizer_name}')\n",
    "        print(f'  learning rate: {self.learning_rate}')\n",
    "        print(f'  loss function: {self.loss_function_name}')\n",
    "        print(f'  epochs: {self.epochs}')\n",
    "        print(f'  random seed: {self.random_state}')\n",
    "        print(f'  early stopping patience:  {self.early_stopping_patience}')\n",
    "        print(f'  positive sample size: {self.positive_sample_size}')\n",
    "        print(f'  negative sample size: {self.negative_sample_size}')\n",
    "        if self.cross_validation_split == None or self.cross_validation_split == 1:\n",
    "            print(f'Training Results: ')\n",
    "            print(f'  true positives: {self.train_evaluation[\"true positives\"]}'), \n",
    "            print(f'  true negatives: {self.train_evaluation[\"true negatives\"]}'), \n",
    "            print(f'  false positives: {self.train_evaluation[\"false positives\"]}'), \n",
    "            print(f'  false negatives: {self.train_evaluation[\"false negatives\"]}'), \n",
    "            \n",
    "            print(f'  accuracy: {self.train_evaluation[\"accuracy\"]}'), \n",
    "            print(f'  precision: {self.train_evaluation[\"precision\"]}'), \n",
    "            print(f'  recall: {self.train_evaluation[\"recall\"]}'), \n",
    "            print(f'  specificity: {self.train_evaluation[\"specificity\"]}'), \n",
    "\n",
    "            print(f'  loss: {self.train_evaluation[\"loss\"]}'), \n",
    "            print(f'  area under curve: {self.train_evaluation[\"area-under-curve\"]}'), \n",
    "            print(f'  precision recall curve: {self.train_evaluation[\"precision-recall-curve\"]}'), \n",
    "            \n",
    "            if self.test_evaluation != None:\n",
    "                print(f'Test Results: ')\n",
    "                print(f'  true positives: {self.test_evaluation[\"true positives\"]}'), \n",
    "                print(f'  true negatives: {self.test_evaluation[\"true negatives\"]}'), \n",
    "                print(f'  false positives: {self.test_evaluation[\"false positives\"]}'), \n",
    "                print(f'  false negatives: {self.test_evaluation[\"false negatives\"]}'), \n",
    "                \n",
    "                print(f'  accuracy: {self.test_evaluation[\"accuracy\"]}'), \n",
    "                print(f'  precision: {self.test_evaluation[\"precision\"]}'), \n",
    "                print(f'  recall: {self.test_evaluation[\"recall\"]}'), \n",
    "                print(f'  specificity: {self.test_evaluation[\"specificity\"]}'), \n",
    "    \n",
    "                print(f'  loss: {self.test_evaluation[\"loss\"]}'), \n",
    "                print(f'  area under curve: {self.test_evaluation[\"area-under-curve\"]}'), \n",
    "                print(f'  precision recall curve: {self.test_evaluation[\"precision-recall-curve\"]}'), \n",
    "        else: \n",
    "            print(f'  cross-validation-split: {self.cross_validation_split}')\n",
    "            print(f'K-Fold Mean Training Results: ')\n",
    "            print(f'  true positives: {self.kfold_train_evaluation[\"true positives\"]}'), \n",
    "            print(f'  true negatives: {self.kfold_train_evaluation[\"true negatives\"]}'), \n",
    "            print(f'  false positives: {self.kfold_train_evaluation[\"false positives\"]}'), \n",
    "            print(f'  false negatives: {self.kfold_train_evaluation[\"false negatives\"]}'), \n",
    "            \n",
    "            print(f'  accuracy: {self.kfold_train_evaluation[\"accuracy\"]}'), \n",
    "            print(f'  precision: {self.kfold_train_evaluation[\"precision\"]}'), \n",
    "            print(f'  recall: {self.kfold_train_evaluation[\"recall\"]}'), \n",
    "            print(f'  specificity: {self.kfold_train_evaluation[\"specificity\"]}'), \n",
    "\n",
    "            print(f'  loss: {self.kfold_train_evaluation[\"loss\"]}'), \n",
    "            print(f'  area under curve: {self.kfold_train_evaluation[\"area-under-curve\"]}'), \n",
    "            print(f'  precision recall curve: {self.kfold_train_evaluation[\"precision-recall-curve\"]}'), \n",
    "            \n",
    "            print(f'K-Fold Mean Test Results: ')\n",
    "            print(f'  true positives: {self.kfold_test_evaluation[\"true positives\"]}'), \n",
    "            print(f'  true negatives: {self.kfold_test_evaluation[\"true negatives\"]}'), \n",
    "            print(f'  false positives: {self.kfold_test_evaluation[\"false positives\"]}'), \n",
    "            print(f'  false negatives: {self.kfold_test_evaluation[\"false negatives\"]}'), \n",
    "            \n",
    "            print(f'  accuracy: {self.kfold_test_evaluation[\"accuracy\"]}'), \n",
    "            print(f'  precision: {self.kfold_test_evaluation[\"precision\"]}'), \n",
    "            print(f'  recall: {self.kfold_test_evaluation[\"recall\"]}'), \n",
    "            print(f'  specificity: {self.kfold_test_evaluation[\"specificity\"]}'), \n",
    "\n",
    "            print(f'  loss: {self.kfold_test_evaluation[\"loss\"]}'), \n",
    "            print(f'  area under curve: {self.kfold_test_evaluation[\"area-under-curve\"]}'), \n",
    "            print(f'  precision recall curve: {self.kfold_test_evaluation[\"precision-recall-curve\"]}'),\n",
    "        print(f' ')\n",
    "            \n",
    "        if show_history:\n",
    "            if self.cross_validation_split == None or self.cross_validation_split == 1:\n",
    "                plot_training_history(self.fit_history)\n",
    "            else: \n",
    "                for idx, history in enumerate(self.kfold_histories):\n",
    "                    print(f'Fold {idx+1}/{len(self.kfold_histories)}')\n",
    "                    plot_training_history(history)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4354383-c24e-4325-b3ac-551f7a7b1eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, true_positive_data=positive_class_data, true_negative_data=negative_class_data):\n",
    "    true_pos_predictions, true_neg_predictions = evaluate_authentication(classifier, true_positive_data, true_negative_data)\n",
    "    \n",
    "    return {\n",
    "        'optimizer': classifier.optimizer_name,\n",
    "        'loss function': classifier.loss_function_name,\n",
    "        'learning rate': classifier.learning_rate,\n",
    "        'random state': classifier.random_state,\n",
    "        'epochs (max)': classifier.max_epochs,\n",
    "        'early stopping patience': classifier.early_stopping_patience,\n",
    "        'epochs trained': classifier.epochs,\n",
    "        'k-fold split': classifier.cross_validation_split,\n",
    "        'positive sample size': classifier.positive_sample_size,\n",
    "        'negative sample size': classifier.negative_sample_size,\n",
    "    \n",
    "        'true positives (train)': classifier.kfold_train_evaluation['true positives'], \n",
    "        'true positives (test)': classifier.kfold_test_evaluation['true positives'], \n",
    "        'true negatives (train)': classifier.kfold_train_evaluation['true negatives'], \n",
    "        'true negatives (test)': classifier.kfold_test_evaluation['true negatives'], \n",
    "        'false positives (train)': classifier.kfold_train_evaluation['false positives'], \n",
    "        'false positives (test)': classifier.kfold_test_evaluation['false positives'], \n",
    "        'false negatives (train)': classifier.kfold_train_evaluation['false negatives'], \n",
    "        'false negatives (test)': classifier.kfold_test_evaluation['false negatives'], \n",
    "        \n",
    "        'accuracy (train)': classifier.kfold_train_evaluation['accuracy'], \n",
    "        'accuracy (test)': classifier.kfold_test_evaluation['accuracy'],\n",
    "        'precision (train)': classifier.kfold_train_evaluation['precision'], \n",
    "        'precision (test)': classifier.kfold_test_evaluation['precision'],\n",
    "        'recall (train)': classifier.kfold_train_evaluation['recall'], \n",
    "        'recall (test)': classifier.kfold_test_evaluation['recall'],\n",
    "        'specificity (train)': classifier.kfold_train_evaluation['specificity'], \n",
    "        'specificity (test)': classifier.kfold_test_evaluation['specificity'],\n",
    "    \n",
    "        'loss (train)': classifier.kfold_train_evaluation['loss'], \n",
    "        'loss (test)': classifier.kfold_test_evaluation['loss'],\n",
    "        'area-under-curve (train)': classifier.kfold_train_evaluation['area-under-curve'], \n",
    "        'area-under-curve (test)': classifier.kfold_test_evaluation['area-under-curve'],\n",
    "        'precision-recall-curve (train)': classifier.kfold_train_evaluation['precision-recall-curve'], \n",
    "        'precision-recall-curve (test)': classifier.kfold_test_evaluation['precision-recall-curve'],\n",
    "    \n",
    "        'autoencoder': f'{classifier.autoencoder.optimizer_name} (lr={classifier.autoencoder.learning_rate}) - {classifier.autoencoder.loss_function_name} - seed={classifier.autoencoder.random_state}', \n",
    "        'ae l2 (augmented)': classifier.autoencoder.euclidean_dist_positive_class, \n",
    "        'ae l2 delta': classifier.autoencoder.euclidean_dist_delta, \n",
    "        'ae l2 ratio': classifier.autoencoder.euclidean_dist_ratio,\n",
    "\n",
    "        'true positive predictions': true_pos_predictions,\n",
    "        'true negative predictions': true_neg_predictions,   \n",
    "    }\n",
    "\n",
    "#@background # concurrency breaks reproducability!\n",
    "def train_classifier(classifier, index):\n",
    "    start = time.time()    \n",
    "    print(f'Starting initialization on classifier #{index}.')\n",
    "    classifier.initialize()\n",
    "    print(f'Finished training classifier #{index} after {(time.time() - start):.2f} seconds.')\n",
    "    \n",
    "    return evaluate_classifier(classifier)\n",
    "\n",
    "def cycle_eval_classifiers(autoencoders, optimizers, loss_functions, learning_rates, random_states_count=20, cross_validation_split=5, output=None):     \n",
    "    classifiers = []\n",
    "    classifier_results = []\n",
    "    \n",
    "    autoencoders_count = len(autoencoders)\n",
    "    optimizers_count = len(optimizers)\n",
    "    loss_functions_count = len(loss_functions)\n",
    "    learning_rates_count = len(learning_rates)\n",
    "\n",
    "    total_training_cycles = autoencoders_count * optimizers_count * loss_functions_count * learning_rates_count * random_states_count\n",
    "\n",
    "    print(f'Starting training & evaluation for {autoencoders_count*optimizers_count*loss_functions_count*learning_rates_count*random_states_count} classifiers:')\n",
    "\n",
    "    for ae_idx in range(autoencoders_count):       \n",
    "        autoencoder_cycles = ae_idx * optimizers_count * loss_functions_count * learning_rates_count * random_states_count\n",
    "        ae = autoencoders[ae_idx]\n",
    "        \n",
    "        for opt_idx in range(optimizers_count):       \n",
    "            optimizer_cycles = opt_idx * loss_functions_count * learning_rates_count * random_states_count\n",
    "    \n",
    "            for loss_idx in range(loss_functions_count):           \n",
    "                loss_function_cycles = loss_idx * learning_rates_count * random_states_count\n",
    "                \n",
    "                for learn_idx in range(learning_rates_count):\n",
    "                    learning_rate_cycles = learn_idx * random_states_count\n",
    "    \n",
    "                    for rand_idx in range(random_states_count):       \n",
    "                        random_seed = random.randrange(1000000000)\n",
    "                        current_cycle = 1+rand_idx+learning_rate_cycles+loss_function_cycles+optimizer_cycles+autoencoder_cycles\n",
    "        \n",
    "                        classifier = Classifier(ae, optimizers[opt_idx], learning_rates[learn_idx], \n",
    "                                    loss_functions[loss_idx], epochs=Classifier.DEFAULT_EPOCHS, \n",
    "                                    early_stopping_patience=20, random_state=random_seed,\n",
    "                                    cross_validation_split=5, init=False)\n",
    "\n",
    "                    \n",
    "                        if output != None:\n",
    "                            classifier_evaluation_row_to_csv(train_classifier(classifier, current_cycle), output[0], output[1])\n",
    "                            #classifier.print_evaluation(show_history=False)\n",
    "                            del classifier\n",
    "                            gc.collect()\n",
    "                        else:\n",
    "                            classifier_results.append(train_classifier(classifier, current_cycle))\n",
    "                            classifiers.append(classifier)\n",
    "    \n",
    "    if output == None:\n",
    "        return classifier_results, classifiers\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def classifier_evaluation_row_to_csv(evaluation, filename, path):    \n",
    "    full_filename = f'{path}/{filename}.csv'\n",
    "    if os.path.exists(full_filename) == False:\n",
    "        with open(full_filename, 'w', newline='') as file:\n",
    "            csv.DictWriter(file, delimiter=\";\", fieldnames=evaluation.keys()).writeheader()\n",
    "     \n",
    "    with open(full_filename, 'a', newline='') as file:\n",
    "        csv.DictWriter(file, delimiter=\";\", fieldnames=evaluation.keys()).writerow(evaluation)\n",
    "\n",
    "def classifier_evaluation_to_csv(evaluation, filename, path):    \n",
    "    dataframe = pd.DataFrame.from_dict(evaluations)   \n",
    "    timestamp = str(datetime.datetime.today()).replace(' ', '_').replace(':', '-').split(\".\")[0]\n",
    "    \n",
    "    dataframe.to_csv(f'{path}/{filename}__{timestamp}.csv', sep=';', decimal=',', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e00047f-d258-40b3-836d-9b3eb476e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced optimizers and loss functions through previous cyclic evaluations\n",
    "def find_classifier_hyperparameters(autoencoders, direct_to_file=False):\n",
    "    optimizers = [\n",
    "        #'Adam',\n",
    "        'AdamW',\n",
    "        #'Lion',\n",
    "        'Adadelta', # very slow learning!\n",
    "        'Adagrad',\n",
    "        'Adamax',\n",
    "        'Ftrl',\n",
    "        'Nadam',\n",
    "        'RMSprop',\n",
    "        'SGD',\n",
    "    ]\n",
    "    \n",
    "    loss_functions = [\n",
    "        'BinaryCrossentropy',\n",
    "        'WeightedBinaryCrossentropy',\n",
    "        'Huber',\n",
    "        'MeanSquaredError',\n",
    "        'Poisson',\n",
    "        'Hinge',\n",
    "        #'CosineSimilarity',\n",
    "        #'MeanSquaredLogarithmicError',\n",
    "        #'MeanAbsoluteError',\n",
    "        #'MeanAbsolutePercentageError',\n",
    "    ]\n",
    "\n",
    "    EVALUATIONS_OUTPUT_PATH = \"evaluations\"\n",
    "    if not os.path.exists(EVALUATIONS_OUTPUT_PATH):\n",
    "        os.makedirs(EVALUATIONS_OUTPUT_PATH)\n",
    "\n",
    "    filename = 'classifier_evaluations_phase-1'\n",
    "    direct_output = (filename, EVALUATIONS_OUTPUT_PATH) if direct_to_file else None\n",
    "    \n",
    "    learning_rates = [0.05, 0.03, 0.01]        \n",
    "    evaluation_results, classifiers = cycle_eval_classifiers(autoencoders, optimizers, loss_functions, learning_rates, random_states_count=1, output=direct_output)\n",
    "\n",
    "    return evaluation_results, classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2482ae91-05fa-4b00-b3ea-99356f4a7f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_classifier_hyperparameters(autoencoders)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
