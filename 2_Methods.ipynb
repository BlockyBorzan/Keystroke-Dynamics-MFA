{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75beb7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CPUs: 12\n",
      "Configured user-id: 999\n",
      "Configured password: trustno1\n",
      "Positive class data: 100 samples with 36 features\n"
     ]
    }
   ],
   "source": [
    "%run \"./1_Config.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cd1e27-47cf-434e-84b6-b85b53a78084",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Async Wrapper Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "490ba147-0399-4aeb-a597-c35d20cb7575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def background(f):\n",
    "    def wrapped(*args, **kwargs):\n",
    "        return asyncio.get_event_loop().run_in_executor(None, f, *args, **kwargs)\n",
    "\n",
    "    return wrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401c73fe-f7ea-48b1-a451-2ec680159eaa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Random Repeatability Reset Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5112e715-39a6-49d8-848c-34f08cadb53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Requires sequential computations on one CPU, multithreading can break repeatability!\n",
    "def reset_random_state(seed_value=0): \n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value) \n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    random.seed(seed_value) \n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    tf.compat.v1.set_random_seed(3)\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc38cb36-5941-4451-801f-86d3d51fca5a",
   "metadata": {},
   "source": [
    "### Data Util Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adb1122d-5396-4d43-9bcf-06382d649e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def dict_mean(dictionaries):\n",
    "    result_dict = {}\n",
    "    for key in dictionaries[0].keys():\n",
    "        result_dict[key] = sum(dictionary[key] for dictionary in dictionaries) / len(dictionaries)\n",
    "    return result_dict\n",
    "    \n",
    "def is_float(num):\n",
    "    try:\n",
    "        floatnum = float(num)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "        \n",
    "def synthesize_normal(data, samples=None):\n",
    "    if samples == None: \n",
    "        samples = len(data)\n",
    "        \n",
    "    means = np.mean(data, axis=0)\n",
    "    stds = np.std(data, axis=0)\n",
    "    \n",
    "    return np.array([[np.random.normal(mean, std) for mean, std in tuple(zip(means, stds))] for i in range(samples)])\n",
    "\n",
    "def synthesize_dissimilar(data, samples=None):\n",
    "    if samples == None: \n",
    "        samples = len(data)\n",
    "    \n",
    "    (normalized, maxima, minima) = normalize(data)\n",
    "    means = np.mean(normalized, axis=0)\n",
    "    average_mse = np.mean((normalized-means)**2, axis=1).mean()\n",
    "    \n",
    "    uniform_data = []\n",
    "    for count in range(samples):\n",
    "        sample = [np.random.uniform(0, 1) for i in range(len(means))] # TODO: generate samples in the range of 0 to 2*max(feature)\n",
    "        error = np.mean((sample-means)**2)\n",
    "        while error < np.sqrt(average_mse/4):\n",
    "            sample = [np.random.uniform(0, 1) for i in range(len(means))]\n",
    "            error = np.mean((sample-means)**2)\n",
    "            \n",
    "        uniform_data.append(sample)\n",
    "\n",
    "    denormalized = denormalize(np.array(uniform_data), maxima, minima)\n",
    "    \n",
    "    return denormalized\n",
    "\n",
    "def sample_rows(data, n):\n",
    "    if(n > len(data)):\n",
    "        raise ValueError(f'Array has fewer rows ({len(data)}) than requested number of samples {n}!')\n",
    "    \n",
    "    return data[np.random.choice(len(data), n, replace=False), :]\n",
    "\n",
    "def normalize(data): \n",
    "    maxima=np.max(data, axis=0)\n",
    "    minima=np.min(data, axis=0)\n",
    "\n",
    "    normalized = np.array(np.transpose([(data[:,idx] - minimum) / (maximum - minimum) for idx, (minimum, maximum) in enumerate(tuple(zip(minima, maxima)))]))\n",
    "\n",
    "    return (normalized, maxima, minima)\n",
    "\n",
    "def denormalize(data, maxima, minima):\n",
    "    return np.array(np.transpose([data[:,idx] * (maximum - minimum) + minimum for idx, (minimum, maximum) in enumerate(tuple(zip(minima, maxima)))]))\n",
    "\n",
    "def augment(data, target_n):\n",
    "    (normalized, maxima, minima) = normalize(data)\n",
    "    synthesized = synthesize_normal(normalized, target_n)\n",
    "    \n",
    "    return denormalize(synthesized, maxima, minima)\n",
    "\n",
    "def drop_outliers(data, target_col, outlier_count=6, outlier_ratio=None):\n",
    "    if outlier_count is not None and outlier_ratio is not None:\n",
    "        raise ValueError(f'Both \\'outlier_count\\' and \\'outlier_ratio\\' specified! Pick only one.')\n",
    "        \n",
    "    drop_count = 0\n",
    "    if outlier_ratio is not None:\n",
    "        drop_count = int(outlier_ratio * len(data_col) / 2)\n",
    "    elif outlier_count is not None:\n",
    "        if outlier_count % 2 != 0:\n",
    "            warnings.warn('The parameter \\'outlier_count\\' is not even. Actually removed outliers will be rounded down to nearest even number (half on each side).')\n",
    "        drop_count = int(outlier_count / 2.0)\n",
    "    else: \n",
    "        warnings.warn(f'Neither \\'outlier_count\\' nor \\'outlier_ratio\\' specified! No outliers will be removed.')\n",
    "        return data_col\n",
    "    \n",
    "    if drop_count <= 0:\n",
    "        warnings.warn(f'Neither \\'outlier_count\\' nor \\'outlier_ratio\\' specified! No outliers will be removed.')\n",
    "        return\n",
    "    \n",
    "    data.sort_values(by=[target_col], inplace=True)\n",
    "    data.drop(data.head(drop_count).index, inplace=True)\n",
    "    data.drop(data.tail(drop_count).index, inplace=True)\n",
    "\n",
    "def create_dataframe(target_data, target_class, data_cols):\n",
    "    dataframe = pd.DataFrame(target_data, columns=data_cols)\n",
    "    dataframe[CLASS_COL] = target_class\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e82807f-04ad-4b22-ad45-56c564848483",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Plotting & Data Visualization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69caf37d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TARGET_CLASS_USER_ID' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_data\u001b[39m(data, category_col, target_cols, col_title_gen, ylabel, \n\u001b[0;32m      2\u001b[0m             title, legend_title, file, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg/data-visualisations\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      3\u001b[0m             plot_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatapoints\u001b[39m\u001b[38;5;124m'\u001b[39m, keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m----> 4\u001b[0m             target_category\u001b[38;5;241m=\u001b[39m\u001b[43mTARGET_CLASS_USER_ID\u001b[49m, class_categories\u001b[38;5;241m=\u001b[39mCLASS_CATEGORIES,\n\u001b[0;32m      5\u001b[0m             display\u001b[38;5;241m=\u001b[39mDISPLAY_PLOTS, save\u001b[38;5;241m=\u001b[39mSAVE_PLOTS, transparency\u001b[38;5;241m=\u001b[39mTRANSPARENT_BACKGROUND,\n\u001b[0;32m      6\u001b[0m             width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, cols_share_y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, trim_outliers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m display \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save:\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TARGET_CLASS_USER_ID' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_data(data, category_col, target_cols, col_title_gen, ylabel, \n",
    "            title, legend_title, file, path=f'img/data-visualisations', \n",
    "            plot_type='datapoints', keys=None,\n",
    "            target_category=TARGET_CLASS_USER_ID, class_categories=CLASS_CATEGORIES,\n",
    "            display=DISPLAY_PLOTS, save=SAVE_PLOTS, transparency=TRANSPARENT_BACKGROUND,\n",
    "            width=None, height=8, cols_share_y=True, trim_outliers=False):\n",
    "    if not display and not save:\n",
    "        return\n",
    "    \n",
    "    if isinstance(target_cols, str):\n",
    "        # target_cols is regex -> get real target cols\n",
    "        target_col_regex = re.compile(target_cols)\n",
    "        target_cols = [col for col in data.columns.values if target_col_regex.match(col)]\n",
    "    \n",
    "    if len(target_cols) == 0:\n",
    "        raise ValueError(f'No target columns found to plot.')\n",
    "        \n",
    "    plot_columns = [category_col, *target_cols]    \n",
    "    for col_name in plot_columns:\n",
    "        if col_name not in data.columns:\n",
    "            raise ValueError(f'Invalid column name: {col_name}.')\n",
    "            \n",
    "    if keys is None:\n",
    "        keys = get_keys(data)\n",
    "\n",
    "    target_data=data[plot_columns].copy(deep=True)\n",
    "    if class_categories == [] and target_category != None:\n",
    "        target_data = target_data[target_data[category_col] == target_category]\n",
    "        target_category = None\n",
    "    \n",
    "    if target_category is not None:\n",
    "        target_data.loc[target_data[category_col] != target_category, [category_col]] = class_categories[1]\n",
    "        target_data.loc[target_data[category_col] == target_category, [category_col]] = class_categories[0]\n",
    "    else:\n",
    "        for category in sorted(target_data[category_col].unique()):\n",
    "            target_data.loc[target_data[category_col] == category, [category_col]] = f'{category}'\n",
    "    \n",
    "    unique_categories = sorted(target_data[category_col].unique())    \n",
    "    order = class_categories \n",
    "    if target_category is None and len(order) == 0:\n",
    "        order = [str(i) for i in sorted([int(j) for j in unique_categories])] if all([is_float(category) for category in unique_categories]) else [str(i) for i in sorted(unique_categories)]\n",
    "    \n",
    "    plt.close('all')\n",
    "    plt.ioff()\n",
    "    fig = plt.figure()\n",
    "\n",
    "    plot_data = target_data.copy(deep=True)\n",
    "    legend_artists = None\n",
    "    if plot_type == 'line':\n",
    "        # ignoring trim outliers      \n",
    "        col_titles = [col_title_gen(keys, col_idx) if callable(col_title_gen) else col_title_gen for col_idx, _ in enumerate(target_cols)]\n",
    "\n",
    "        if width == None:\n",
    "            width = len(col_titles)*2\n",
    "        \n",
    "        ax = plt.gca()\n",
    "        legend_artists = plot_lines(ax, plot_data[[category_col, *target_cols]], category_col, target_cols, order, col_titles=col_titles, trim_outliers=drop_outliers)\n",
    "\n",
    "        ax.get_legend().set(visible=False)\n",
    "\n",
    "        ax.set_xlabel('Keys')\n",
    "        ax.set_ylabel('Seconds')\n",
    "        ax.xaxis.set_ticks(ax.get_xticks(), labels=col_titles)\n",
    "\n",
    "    else:\n",
    "        grid = gridspec.GridSpec(1, len(target_cols), wspace=0.0, hspace=0.0) # 1 row, no spacing between axes\n",
    "\n",
    "        if width == None: \n",
    "            width = 2*len(target_cols)*len(plot_data[category_col].unique())\n",
    "        \n",
    "        for col_idx, target_column in enumerate(target_cols):\n",
    "            sharey = None if (col_idx==0 or cols_share_y) else plt.subplot(grid[0, 0])\n",
    "            ax = plt.subplot(grid[0, col_idx], sharey=sharey)\n",
    "            col_title = col_title_gen(keys, col_idx) if callable(col_title_gen) else col_title_gen\n",
    "            \n",
    "            if trim_outliers:\n",
    "                drop_outliers(plot_data, target_column)\n",
    "            \n",
    "            mean_var_vals = [(plot_data[plot_data[category_col] == category][target_column].mean(), plot_data[plot_data[category_col] == category][target_column].std()) for category in order]\n",
    "            x_tick_labels = [f'μ: {mean:.3f}\\nσ: {std:.3f}' for mean, std in mean_var_vals]\n",
    "            \n",
    "            match plot_type:\n",
    "                case 'datapoints':\n",
    "                    legend_artists = plot_column_datapoints(ax, plot_data[[category_col, target_column]], category_col, target_column, order, col_title=col_title, trim_outliers=drop_outliers)\n",
    "                case 'violin':\n",
    "                    legend_artists = plot_column_violins(ax, plot_data[[category_col, target_column]], category_col, target_column, order, col_title=col_title, trim_outliers=drop_outliers)\n",
    "                case 'overlap':\n",
    "                    legend_artists = plot_column_overlap(ax, plot_data[[category_col, target_column]], category_col, target_column, order, col_title=col_title, trim_outliers=drop_outliers)\n",
    "                case _:\n",
    "                    raise ValueError(f'Invalid \\'plot_type\\' parameter: {plot_type}.')    \n",
    "                    \n",
    "            ax.xaxis.set_ticks(ax.get_xticks(), labels=x_tick_labels)\n",
    "            if col_idx!=0:\n",
    "                ax.yaxis.set_ticks([])\n",
    "    \n",
    "            ax.get_legend().set(visible=False)\n",
    "            ax.margins(x=1.0/len(unique_categories))\n",
    "    \n",
    "            ax.set_xlabel(None)\n",
    "            if col_idx==0:\n",
    "                ax.set_ylabel(ylabel, fontsize=14)\n",
    "            else:\n",
    "                ax.set_ylabel(None)\n",
    "    \n",
    "            ax.set_title(col_title)        \n",
    "        \n",
    "        \n",
    "    plt.suptitle(title, y=0.975, fontsize=18)    \n",
    "    handles, labels = legend_artists\n",
    "    fig.legend(title=legend_title, handles=handles, labels=labels, loc=\"center left\", bbox_to_anchor=(0.91, 0.5))\n",
    "\n",
    "    if height == None: \n",
    "        height = 8\n",
    "        \n",
    "    fig.set_size_inches(width, height)\n",
    "        \n",
    "    if save:\n",
    "        save_plot(f'{file}', path, transparency)\n",
    "    if display:\n",
    "        display_plot()\n",
    "\n",
    "def plot_lines(ax, data, category_col, target_cols, order, trim_outliers=False, col_titles = None):  \n",
    "    unique_categories = np.sort(data[category_col].unique()).astype(str)\n",
    "   \n",
    "    dataset = []\n",
    "    for category in unique_categories:\n",
    "        category_data = data[data[category_col] == category]\n",
    "        for idx, target_col in enumerate(target_cols):\n",
    "            dataset.append({\n",
    "                category_col: category,\n",
    "                'x': target_col,\n",
    "                'y': np.mean(category_data[target_col]),\n",
    "                'min': np.mean(category_data[target_col])-np.abs(np.std(category_data[target_col])),\n",
    "                'max': np.mean(category_data[target_col])+np.abs(np.std(category_data[target_col]))\n",
    "            })\n",
    "        \n",
    "    dataframe = pd.DataFrame(dataset, columns=[category_col, 'x', 'y', 'min', 'max'])\n",
    "    sns.lineplot(ax=ax, data=dataframe, x='x', y='y', hue=category_col, hue_order=order, palette=\"Set2\")\n",
    "\n",
    "    if len(order) <= 2:\n",
    "        for category in order:\n",
    "            ax.fill_between(range(len(col_titles)), \n",
    "                            dataframe[dataframe[category_col] == category]['min'], \n",
    "                            dataframe[dataframe[category_col] == category]['max'], \n",
    "                            alpha=0.4)\n",
    "        \n",
    "    return ax.get_legend_handles_labels()\n",
    "    \n",
    "def plot_column_datapoints(ax, data, category_col, target_col, order, trim_outliers=False, col_title = None):  \n",
    "    sns.stripplot(ax=ax, data=data, x=category_col, y=target_col, hue=category_col, order=order, hue_order=order, marker='o', size=10, alpha=.35, jitter=True, palette=\"Set2\")    \n",
    "    \n",
    "    return ax.get_legend_handles_labels()\n",
    "    \n",
    "def plot_column_violins(ax, data, category_col, target_col, order, trim_outliers=False, col_title = None):                \n",
    "    sns.violinplot(ax=ax, data=data, x=category_col, y=target_col, scale='width', dodge=False, hue=category_col, order=order, hue_order=order, scale_hue=False, palette='Set2')\n",
    "    \n",
    "    return ax.get_legend_handles_labels()\n",
    "    \n",
    "def plot_column_overlap(ax, data, category_col, target_col, order, trim_outliers=False, col_title = None):            \n",
    "    sns.violinplot(ax=ax, data=data, x=category_col, y=target_col, hue=category_col, order=order, hue_order=order, palette='Set2', scale='width', dodge=False, inner=None)\n",
    "    ret = ax.get_legend_handles_labels()\n",
    "    \n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    for violin in ax.collections:\n",
    "        bbox = violin.get_paths()[0].get_extents()\n",
    "        x0, y0, width, height = bbox.bounds\n",
    "        violin.set_clip_path(plt.Rectangle((x0, y0), width / 2, height, transform=ax.transData))\n",
    "    \n",
    "    old_len_collections = len(ax.collections)\n",
    "    sns.stripplot(ax=ax, data=data, x=category_col, y=target_col, hue=category_col, order=order, hue_order=order, palette='Set2', dodge=False)\n",
    "    for dots in ax.collections[old_len_collections:]:\n",
    "        dots.set_offsets(dots.get_offsets() + np.array([0.12, 0]))\n",
    "    \n",
    "    sns.boxplot(ax=ax, data=data, x=category_col, y=target_col, hue=category_col, order=order, hue_order=order, palette='Set2', saturation=1, width=0.3, boxprops={'zorder': 3, 'facecolor': 'none'}, dodge=False)\n",
    "    \n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e827ba4b-4677-4c7f-86fc-832427ca72bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data Visualization Util Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3be5c301-04ac-40fc-8104-a51afeeec2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_plot():\n",
    "    plt.show()\n",
    "    plt.close('all')\n",
    "\n",
    "def save_plot(file, path, transparency):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    plt.savefig(f'{path}/{file}.png', format='png', dpi=100, transparent=transparency)\n",
    "    \n",
    "def concat_images(file, images, path, orientation='vertical'):\n",
    "    if len(images) < 2:\n",
    "        return \n",
    "    \n",
    "    images = [Image.open(f'{path}/{image}.png') for image in images]\n",
    "    widths, heights = zip(*(i.size for i in images))\n",
    "    \n",
    "    match orientation:\n",
    "        case 'horizontal':    \n",
    "            total_width = sum(widths)\n",
    "            max_height = max(heights)\n",
    "\n",
    "            new_im = Image.new('RGBA', (total_width, max_height))\n",
    "\n",
    "            x_offset = 0\n",
    "            for im in images:\n",
    "              new_im.paste(im, (x_offset,0))\n",
    "              x_offset += im.size[0]\n",
    "\n",
    "            new_im.save(filename)\n",
    "        case 'vertical':    \n",
    "            total_height = sum(heights)\n",
    "            max_width = max(widths)\n",
    "\n",
    "            new_im = Image.new('RGBA', (max_width, total_height))\n",
    "            \n",
    "            y_offset = 0\n",
    "            for im in images:\n",
    "              new_im.paste(im, (0,y_offset))\n",
    "              y_offset += im.size[1]\n",
    "\n",
    "            new_im.save(f'{path}/{file}.png')\n",
    "        case _:\n",
    "            raise ValueError(f'Orientation \\'{orientation}\\' is invalid. Use \\'horizontal\\' or \\'vertical\\'.')\n",
    "\n",
    "def get_keys(data):\n",
    "    key_re = re.compile('key\\d+')\n",
    "    target_cols = [col for col in data.columns.values if key_re.match(col)]\n",
    "    return [chr(data.at[0, key_col]) for key_col in target_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3ecd63-1569-45dc-b114-ecb98dab9599",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Feature-Specific Data Visualization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b2fd62b-8017-4e07-93f7-c8e0e51cb52e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col_title_gen_keys = lambda keys, idx : keys[idx].lower().replace(' ', '␣')\n",
    "col_title_gen_relations = lambda keys, idx : f'{keys[idx]}_→_{keys[idx+1]}'.lower().replace(' ', '␣').replace('_', ' ')\n",
    "\n",
    "def plot_d_data(data, password, legend_title = 'class', file = 'press-duration-times', path=f'img/data-visualisations', \n",
    "            plot_type='datapoints', keys=None,\n",
    "            target_category=TARGET_CLASS_USER_ID, class_categories=CLASS_CATEGORIES,\n",
    "            display=DISPLAY_PLOTS, save=SAVE_PLOTS, transparency=TRANSPARENT_BACKGROUND,\n",
    "            width=None, height=8, cols_share_y=True, trim_outliers=False):\n",
    "    plot_data(data, 'userId', DURATION_COL_PATTERN, col_title_gen_keys, 'seconds', \n",
    "              f'Press-Duration Times: \\'{password}\\'', legend_title,\n",
    "              file, path, plot_type, keys, target_category, class_categories,\n",
    "              display, save, transparency, width, height, cols_share_y, trim_outliers)\n",
    "    \n",
    "def plot_pp_data(data, password, legend_title = 'class', file = 'press-press-times', path=f'img/data-visualisations', \n",
    "            plot_type='datapoints', keys=None,\n",
    "            target_category=TARGET_CLASS_USER_ID, class_categories=CLASS_CATEGORIES,\n",
    "            display=DISPLAY_PLOTS, save=SAVE_PLOTS, transparency=TRANSPARENT_BACKGROUND,\n",
    "            width=None, height=8, cols_share_y=True, trim_outliers=False):\n",
    "    plot_data(data, 'userId', PP_COL_PATTERN, col_title_gen_relations, 'seconds', \n",
    "              f'Press-Press Times: \\'{password}\\'', legend_title,\n",
    "              file, path, plot_type, keys, target_category, class_categories,\n",
    "              display, save, transparency, width, height, cols_share_y, trim_outliers)\n",
    "\n",
    "def plot_pr_data(data, password, legend_title = 'class', file = 'press-release-times', path=f'img/data-visualisations', \n",
    "            plot_type='datapoints', keys=None,\n",
    "            target_category=TARGET_CLASS_USER_ID, class_categories=CLASS_CATEGORIES,\n",
    "            display=DISPLAY_PLOTS, save=SAVE_PLOTS, transparency=TRANSPARENT_BACKGROUND,\n",
    "            width=None, height=8, cols_share_y=True, trim_outliers=False):\n",
    "    plot_data(data, 'userId', PR_COL_PATTERN, col_title_gen_relations, 'seconds', \n",
    "              f'Press-Release Times: \\'{password}\\'', legend_title,\n",
    "              file, path, plot_type, keys, target_category, class_categories,\n",
    "              display, save, transparency, width, height, cols_share_y, trim_outliers)\n",
    "\n",
    "def plot_rp_data(data, password, legend_title = 'class', file = 'release-press-times', path=f'img/data-visualisations', \n",
    "            plot_type='datapoints', keys=None,\n",
    "            target_category=TARGET_CLASS_USER_ID, class_categories=CLASS_CATEGORIES,\n",
    "            display=DISPLAY_PLOTS, save=SAVE_PLOTS, transparency=TRANSPARENT_BACKGROUND,\n",
    "            width=None, height=8, cols_share_y=True, trim_outliers=False):\n",
    "    plot_data(data, 'userId', RP_COL_PATTERN, col_title_gen_relations, 'seconds', \n",
    "              f'Release-Press Times: \\'{password}\\'', legend_title,\n",
    "              file, path, plot_type, keys, target_category, class_categories,\n",
    "              display, save, transparency, width, height, cols_share_y, trim_outliers)\n",
    "\n",
    "def plot_rr_data(data, password, legend_title = 'class', file = 'release-release-times', path=f'img/data-visualisations', \n",
    "            plot_type='datapoints', keys=None,\n",
    "            target_category=TARGET_CLASS_USER_ID, class_categories=CLASS_CATEGORIES,\n",
    "            display=DISPLAY_PLOTS, save=SAVE_PLOTS, transparency=TRANSPARENT_BACKGROUND,\n",
    "            width=None, height=8, cols_share_y=True, trim_outliers=False):\n",
    "    plot_data(data, 'userId', RR_COL_PATTERN, col_title_gen_relations, 'seconds', \n",
    "              f'Release-Release Times: \\'{password}\\'', legend_title,\n",
    "              file, path, plot_type, keys, target_category, class_categories,\n",
    "              display, save, transparency, width, height, cols_share_y, trim_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f86d056-6fc6-4dc0-976c-cc481dc496ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Alternative Input Data Visualization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95794ae2-3d8b-4e64-9f2f-e09098621bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_key_data_errorbars(title, filename, subplot_label_lambda, col_prefix, cols_count, \n",
    "                               group_data, group_labels, lgroup_labels, group_colors, drop_ratio = 0.025): \n",
    "    #data has to be an array\n",
    "    fig = plt.figure()    \n",
    "    grid = gridspec.GridSpec(1, cols_count, wspace=0.0, hspace=0.0)\n",
    "    data_dim = len(group_data)   \n",
    "        \n",
    "    dict_temp = {}\n",
    "    for group_idx in range(data_dim):\n",
    "        dict_temp[group_labels[group_idx]] = []\n",
    "    \n",
    "    cols = copy.deepcopy(dict_temp)\n",
    "    mus = copy.deepcopy(dict_temp)\n",
    "    sigmas = copy.deepcopy(dict_temp)\n",
    "    mins = copy.deepcopy(dict_temp)\n",
    "    maxs = copy.deepcopy(dict_temp)\n",
    "    \n",
    "    ax_min = float(\"inf\")  \n",
    "    for col_idx in range(cols_count): \n",
    "        \n",
    "        sharey = None if col_idx==0 else plt.subplot(grid[0, 0])\n",
    "        ax = plt.subplot(grid[0, col_idx], sharey=sharey)            \n",
    "        \n",
    "        for group_idx, gdata in enumerate(group_data):\n",
    "            col = gdata[col_prefix+str(col_idx)]\n",
    "        \n",
    "            col_mu, col_sigma = col.agg([np.mean, np.std])\n",
    "            col = drop_outliers(col, drop=drop_ratio)\n",
    "            col_min, col_max = np.min(col), np.max(col)\n",
    "\n",
    "            col = np.array(col)\n",
    "            col_mu = np.array(col_mu)\n",
    "            col_sigma = np.array(col_sigma)\n",
    "            col_min = np.array(col_min)\n",
    "            col_max = np.array(col_max)\n",
    "            col_mid = (col_min + col_max)/2\n",
    "            \n",
    "            ax_min = min(col_min, ax_min)\n",
    "            \n",
    "            ax.errorbar(np.array([group_idx+1]), col_mid, yerr=col_mid-col_min, fmt='none', \n",
    "                capsize=4, ecolor=group_colors[data_dim], elinewidth=2, capthick=2);\n",
    "            ax.errorbar(np.array([group_idx+1]), col_mu, yerr=col_sigma, color=group_colors[group_idx][0], fmt='.', \n",
    "                        capsize=0.0, ecolor=group_colors[group_idx][1], elinewidth=6, markersize=9);\n",
    "            \n",
    "        ax.set_xticks([])\n",
    "        if(col_idx > 0): \n",
    "            plt.setp(ax.get_yticklabels(), visible=False)\n",
    "            plt.tick_params(left = False)\n",
    "        ax.set_title(subplot_label_lambda(col_idx), fontname='Fira Code', fontsize=12, y=1.01)\n",
    "        ax.set_xlim([0, data_dim+1])\n",
    "        ax.spines['top'].set_visible(False)\n",
    "                \n",
    "    plt.suptitle(title, y=1.0, fontsize=18)\n",
    "    \n",
    "    legend_markers = [\n",
    "        Line2D([0], [0], color=group_colors[data_dim], lw=2)\n",
    "    ]\n",
    "    legend_labels = [\n",
    "        '95% of datapoints'\n",
    "    ]\n",
    "    for group_idx in range(data_dim):\n",
    "        legend_markers.append(Line2D([0], [0], \n",
    "             markerfacecolor=group_colors[group_idx][0], \n",
    "             color=group_colors[group_idx][1],\n",
    "             markersize=6, marker='o', lw=6)\n",
    "        )\n",
    "        \n",
    "        legend_labels.append(f'µ, σ ({lgroup_labels[group_idx]})')\n",
    "\n",
    "    fig.legend(legend_markers, legend_labels, fontsize=12, loc='upper center', bbox_to_anchor=(0.5, 0.08), fancybox=True, ncol = 3)\n",
    "    \n",
    "    \n",
    "    display_plot(filename, 16, 6, path='img/data-visualisations')\n",
    "    \n",
    "def display_data_density(title, img_name, cols_prefix, shared_metric, metric_label_lambda, label, max_samples, data, colors, drop_ratio = 0.025):\n",
    "    filtered = data.filter(regex='^'+cols_prefix + '|^userId', axis=1)   \n",
    "    df = pd.DataFrame(columns=['user', 'value', shared_metric])\n",
    "                \n",
    "    for col_idx, col in enumerate(filtered.columns.values): #.iloc[:1].values[0]\n",
    "        if col == 'userId':\n",
    "            continue\n",
    "        kwargs = {shared_metric : lambda idx : col_idx-1}\n",
    "    \n",
    "        series = filtered[['userId', col]].rename(columns = {'userId': 'user', col: 'value'}).assign(**kwargs)\n",
    "\n",
    "        series.loc[series['user'] != MY_USER_ID, ['user']] = 'actor'\n",
    "        series.loc[series['user'] == MY_USER_ID, ['user']] = 'user' \n",
    "        \n",
    "        user_series = series[series['user'] == 'user']\n",
    "        actor_series = drop_outliers(series[series['user'] == 'actor'], by='value', drop=drop_ratio)\n",
    "                \n",
    "        samples = min(max_samples, len(user_series), len(actor_series))\n",
    "        user_series = user_series.sample(samples, random_state=RANDOM_STATE)\n",
    "        actor_series = actor_series.sample(samples, random_state=RANDOM_STATE)\n",
    "                \n",
    "        df = pd.concat([df, user_series], ignore_index = True)\n",
    "        df = pd.concat([df, actor_series], ignore_index = True)  \n",
    "            \n",
    "    bin_min = df['value'].min()\n",
    "    bin_max = df['value'].max()\n",
    "    \n",
    "    if DISPLAY_PLOTS: \n",
    "        grid = sns.FacetGrid(df, col=shared_metric, palette=\"muted\", hue='user', hue_kws={'color': colors})       \n",
    "        #grid.map_dataframe(sns.histplot, x='value', kde=True, bins=15, binrange=(bin_min, bin_max))\n",
    "        grid.map_dataframe(sns.kdeplot, x='value', fill=True)\n",
    "        grid.set_axis_labels(label, \"Density\")\n",
    "        grid.figure.subplots_adjust(wspace=0, hspace=0)    \n",
    "        grid.fig.suptitle(title, fontsize=18, y=1.1)   \n",
    "        grid.add_legend(title='', loc='lower center', fancybox=True, ncol = 2, bbox_to_anchor=(0.5, -0.1))\n",
    "                \n",
    "        for col_idx, col in enumerate(filtered.columns.values):\n",
    "            if col == 'userId':\n",
    "                continue\n",
    "            ax = grid.facet_axis(0, col_idx-1)\n",
    "            ax.set_title(metric_label_lambda(data, col_idx), fontname='Fira Code', fontsize=12, y=1.0)\n",
    "            ax.spines['right'].set_visible(True)\n",
    "        \n",
    "        if SAVE_PLOTS:\n",
    "            path = 'img/data-visualisations'\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "            plt.savefig(f'{path}/{img_name}.png', format='png', dpi=100, transparent=TRANSPARENT_BACKGROUND)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d57e39-6555-4fd5-b93b-40a10a093e1a",
   "metadata": {},
   "source": [
    "### ML-training History Visualization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fbcdf4-a704-4d3f-b859-f613d09ff519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_loss(fit_history): \n",
    "    print(fit_history)\n",
    "    epochs = len(fit_history.epoch)\n",
    "    hist_dataframe = pd.DataFrame(fit_history.history)\n",
    "    \n",
    "    loss_cols = [col for col in hist_dataframe.columns.values if re.compile('.*loss.*').match(col)]\n",
    "    \n",
    "    pd.DataFrame(hist_dataframe)[loss_cols].plot(\n",
    "        figsize=(8, 5), xlim=[0, epochs-1], grid=True, xlabel=\"Epoch\"\n",
    "    )\n",
    "    plt.legend(loss_cols)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_training_history(classifier_history):\n",
    "    epochs = len(classifier_history.epoch)\n",
    "    hist_dataframe = pd.DataFrame(classifier_history.history)\n",
    "    #print(hist_dataframe)\n",
    "    \n",
    "    loss_cols = [col for col in hist_dataframe.columns.values if re.compile('.*loss.*').match(col)]\n",
    "    val_cols = [col for col in hist_dataframe.columns.values if re.compile('val.*').match(col)]\n",
    "    count_cols = [col for col in hist_dataframe.columns.values if re.compile('.*(t|f).*(p|n).*').match(col) and col not in loss_cols and col not in val_cols]\n",
    "    metric_cols = [col for col in hist_dataframe.columns.values if re.compile('.*').match(col) and col not in loss_cols and col not in count_cols and col not in val_cols]\n",
    "    \n",
    "    pd.DataFrame(hist_dataframe)[loss_cols].plot(\n",
    "        figsize=(8, 5), xlim=[0, epochs-1], grid=True, xlabel=\"Epoch\"\n",
    "    )\n",
    "    plt.legend(loss_cols)\n",
    "    plt.show()\n",
    "    \n",
    "    pd.DataFrame(hist_dataframe)[count_cols].plot(\n",
    "        figsize=(8, 5), xlim=[0, epochs-1], grid=True, xlabel=\"Epoch\" #ylim=[0, samples*0.55], \n",
    "    )\n",
    "    plt.legend(count_cols)\n",
    "    plt.show()\n",
    "    \n",
    "    pd.DataFrame(hist_dataframe)[metric_cols].plot(\n",
    "        figsize=(8, 5), xlim=[0, epochs-1], grid=True, xlabel=\"Epoch\"\n",
    "    )\n",
    "    plt.legend(metric_cols)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f872372-c73e-46a6-a582-dd922356cf0d",
   "metadata": {},
   "source": [
    "### ML-Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d61e5cf-080a-4862-b5a7-851a0a928fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_authentication(classifier, positive_dataset, negative_dataset):\n",
    "    X = positive_dataset[DATA_COLS].values\n",
    "    N = negative_dataset[DATA_COLS].values\n",
    "    \n",
    "    pred_pos = [round(elem) for elem in flatten(classifier.predict(X))]\n",
    "    pred_neg = [round(elem) for elem in flatten(classifier.predict(N))]\n",
    "\n",
    "    absolute_pos = pred_pos.count(positive_class)\n",
    "    absolute_neg = pred_neg.count(negative_class)\n",
    "\n",
    "    percent_pos = float(pred_pos.count(positive_class))/len(pred_pos)\n",
    "    percent_neg = float(pred_neg.count(negative_class))/len(pred_neg)\n",
    "    \n",
    "    print(f'Predicted positives: {absolute_pos}/{len(pred_pos)} ({percent_pos*100:.3f}%)')\n",
    "    print(f'Predicted negatives: {absolute_neg}/{len(pred_neg)} ({percent_neg*100:.3f}%)')\n",
    "\n",
    "    return (percent_pos, percent_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc9b26b-9329-4d10-8d5a-be0c760d3493",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Experimental Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eed07e-5524-49b3-a4e1-cfe453ea4dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment2(data, target_n):\n",
    "    (normalized, maxima, minima) = normalize(data)\n",
    "    synthesized = synthesize_normal(normalized, target_n)\n",
    "\n",
    "    reshaped_means = np.mean(normalized, axis=0).reshape(1, -1)\n",
    "    cosine_similarities = cosine_similarity(reshaped_means, random_generated)[0]\n",
    "    \n",
    "    # Replace samples in dataset B with entirely new data if similarity above threshold\n",
    "    for idx, similarity in enumerate(cosine_similarities):\n",
    "        while similarity < 0.9:\n",
    "            print(\"(SN) similarity too small: \", similarity)\n",
    "            replacement = synthesize_normal2(random_generated, 1)\n",
    "            synthesized[idx] = replacement\n",
    "            similarity = cosine_similarity(reshaped_means, replacement.reshape(1, -1))[0][0]\n",
    "    \n",
    "    return denormalize(synthesized, maxima, minima)\n",
    "\n",
    "\n",
    "def synthesize_dissimilar2(data, samples=None):\n",
    "    if samples == None:\n",
    "        samples = len(data)\n",
    "\n",
    "    (normalized, maxima, minima) = normalize(data)\n",
    "    means = np.mean(normalized, axis=0)\n",
    "    \n",
    "    cosine_similarity_matrix = np.dot(data - means, (data - means).T)\n",
    "    cosine_similarity_matrix /= np.outer(np.linalg.norm(data - means, axis=1), np.linalg.norm(data - means, axis=1))\n",
    "\n",
    "    target_stds = np.std(normalized, axis=0) * 2 # ADJUST THIS AND CHECK\n",
    "\n",
    "    dissimilar_data = np.array([[np.random.normal(mean, std) for mean, std in tuple(zip(means, target_stds))] for i in range(samples)])\n",
    "\n",
    "    pca = PCA(n_components=dissimilar_data.shape[1])\n",
    "    pca.fit(dissimilar_data)\n",
    "    dissimilar_pca = pca.transform(dissimilar_data)\n",
    "    \n",
    "    dissimilar_pca *= (np.mean(means) / np.mean(dissimilar_pca))\n",
    "    dissimilar_denormalized = denormalize(pca.inverse_transform(dissimilar_pca), maxima, minima)\n",
    "    print(dissimilar_denormalized.shape)\n",
    "    \n",
    "    return dissimilar_denormalized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
