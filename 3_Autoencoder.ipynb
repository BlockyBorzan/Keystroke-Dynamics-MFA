{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ee25aa7-96a2-420f-a9a7-5bad9126fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./2_Methods.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45767489-622b-4796-839c-7c9f0856dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder:      \n",
    "    DEFAULT_EPOCHS = 100\n",
    "    \n",
    "    def __init__(self, input_data, password, optimizer_name, learning_rate, loss_function_name,\n",
    "                 metrics=[], layer_count=2, layer_activation='relu', \n",
    "                 random_state = None, epochs=DEFAULT_EPOCHS, early_stopping_patience=None,\n",
    "                 augmentation_factor=10, data_cols=DATA_COLS, verbose=None, init=True):        \n",
    "        if(layer_count < 1):\n",
    "            raise ValueError(f'The minimum number of layers must be 1! (Specified: {layers_count})')\n",
    "            \n",
    "        if random_state == None: \n",
    "            random_state = random.randrange(1000000000)\n",
    "\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.input_data = input_data\n",
    "        self.password = password\n",
    "        self.data_cols = data_cols\n",
    "        self.feature_count = input_data.shape[1]\n",
    "        \n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_function_name = loss_function_name\n",
    "        self.metrics = metrics\n",
    "    \n",
    "        self.layer_count = layer_count\n",
    "        self.layer_activation = layer_activation\n",
    "        self.max_epochs = epochs\n",
    "        self.augmentation_factor = augmentation_factor\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        \n",
    "        self.autoencoder_data = None\n",
    "        self.random_dissimilar_data = None\n",
    "        self.model = None\n",
    "        self.epochs = None\n",
    "\n",
    "        self.fit_history = None\n",
    "        self.trained_class_output = None\n",
    "        self.positive_class_output = None\n",
    "        self.negative_class_output = None\n",
    "\n",
    "        self.initialized = False\n",
    "\n",
    "        if init:\n",
    "            self.initialize()\n",
    "                    \n",
    "    def __reset_state(self, seed):\n",
    "        reset_random_state(seed)\n",
    "\n",
    "    def __generate_training_data(self):\n",
    "        self.autoencoder_data = np.concatenate((self.input_data, augment(self.input_data, len(self.input_data) * self.augmentation_factor)), axis=0)\n",
    "        self.random_dissimilar_data = synthesize_dissimilar(self.input_data, len(self.input_data) * (self.augmentation_factor+1))\n",
    "\n",
    "    def __create_model(self):\n",
    "        encoder_features = []\n",
    "        decoder_features = []\n",
    "    \n",
    "        for layer in range(self.layer_count): \n",
    "            encoder_features.append(int(self.feature_count/2**(layer+1)))\n",
    "            decoder_features.append(int(self.feature_count/2**(layer)))\n",
    "        decoder_features.reverse()\n",
    "\n",
    "        encoder_layers = []\n",
    "        decoder_layers = []\n",
    "\n",
    "        for layer in range(self.layer_count): \n",
    "            encoder_layers.append(tf.keras.layers.Dense(encoder_features[layer], activation=self.layer_activation))\n",
    "            decoder_layers.append(tf.keras.layers.Dense(decoder_features[layer], activation=(None if layer == self.layer_count-1 else self.layer_activation)))                \n",
    "\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.Sequential(encoder_layers), \n",
    "            tf.keras.Sequential(decoder_layers)\n",
    "        ])\n",
    "\n",
    "    def __train(self, X, epochs=DEFAULT_EPOCHS):\n",
    "        # sample 5% random negative data to mix into training\n",
    "        X_neg = self.random_dissimilar_data\n",
    "        \n",
    "        X_train, X_val, _, _ = train_test_split(X, X, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        X_train = np.concatenate((X_train, X_neg[np.random.choice(len(X_neg), int(len(X_neg)*0.05), replace=False)]), axis=0)\n",
    "        X_val = np.concatenate((X_val, X_neg[np.random.choice(len(X_neg), int(len(X_neg)*0.05), replace=False)]), axis=0)\n",
    "\n",
    "        callbacks = []\n",
    "        if self.early_stopping_patience != None:            \n",
    "            callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=self.early_stopping_patience, restore_best_weights=True))\n",
    "        \n",
    "        self.fit_history = self.model.fit(X_train, X_train, epochs=epochs, \n",
    "                                          validation_data=(X_val, X_val),\n",
    "                                          callbacks=callbacks, \n",
    "                                          verbose=self.verbose)\n",
    "\n",
    "        self.epochs = len(self.fit_history.epoch)\n",
    "        \n",
    "        self.trained_class_output = self.predict(self.input_data)\n",
    "        self.positive_class_output = self.predict(self.autoencoder_data)\n",
    "        self.negative_class_output = self.predict(self.random_dissimilar_data)\n",
    "\n",
    "        means=np.mean(self.trained_class_output, axis=0)\n",
    "                \n",
    "        self.euclidean_dist_positive_class = np.linalg.norm(means - np.mean(self.positive_class_output, axis=0)) \n",
    "        self.euclidean_dist_negative_class = np.linalg.norm(means - np.mean(self.negative_class_output, axis=0))\n",
    "        \n",
    "        self.euclidean_dist_delta = abs(self.euclidean_dist_negative_class - self.euclidean_dist_positive_class)\n",
    "        self.euclidean_dist_ratio = self.euclidean_dist_delta / self.euclidean_dist_positive_class if self.euclidean_dist_positive_class > 0 else -1\n",
    "\n",
    "        self.output = {\n",
    "            'meta-info': {\n",
    "                'optimizer': self.optimizer_name,\n",
    "                'learning rate': self.learning_rate,\n",
    "                'loss function': self.loss_function_name,\n",
    "                'random seed': self.random_state,\n",
    "                'epochs (max)': self.max_epochs,\n",
    "                'early stopping': self.early_stopping_patience,\n",
    "                'epochs trained': self.epochs,\n",
    "            },\n",
    "            'output': {\n",
    "                'positive class': self.positive_class_output, \n",
    "                'negative class': self.negative_class_output, \n",
    "                'raw input': self.trained_class_output\n",
    "            },\n",
    "            'euclidean distance evaluation': {\n",
    "                'l2 positive class': self.euclidean_dist_positive_class,\n",
    "                'l2 negative class': self.euclidean_dist_negative_class,\n",
    "                'l2 delta':  self.euclidean_dist_delta,\n",
    "                'l2 ratio': self.euclidean_dist_ratio\n",
    "            },\n",
    "            'history': self.fit_history\n",
    "        }\n",
    "    def initialize(self):\n",
    "        if self.initialized:\n",
    "            raise ValueError(f'Cannot initialize already initialized autoencoder!')\n",
    "            \n",
    "        self.__reset_state(self.random_state)\n",
    "        self.__generate_training_data()\n",
    "                \n",
    "        self.optimizer=globals()[self.optimizer_name](self.learning_rate)\n",
    "        self.loss_function = tf.keras.losses.get(self.loss_function_name)\n",
    "        \n",
    "        self.__create_model()\n",
    "        self.model.compile(\n",
    "            optimizer=self.optimizer,\n",
    "            loss=self.loss_function,\n",
    "            metrics=self.metrics\n",
    "        )\n",
    "        \n",
    "        self.__train(self.autoencoder_data, epochs=self.max_epochs)\n",
    "        self.initialized = True\n",
    "    \n",
    "    def predict(self, X, verbose=None):\n",
    "        return self.model.predict(X, verbose=verbose)\n",
    "\n",
    "    def generate_positive(self, count):\n",
    "        return self.predict(augment(self.input_data, count))\n",
    "\n",
    "    def generate_negative(self, count):\n",
    "        return self.predict(synthesize_dissimilar(self.input_data, count))\n",
    "    \n",
    "    def print_evaluation(self, show_history=False, verbose=False):\n",
    "        print(f'Autoencoder Model:')\n",
    "        if verbose:\n",
    "            print(f'  layers: {self.layer_count}')\n",
    "            print(f'  activation: {self.layer_activation}')\n",
    "            print(f'  feature count: {self.feature_count}')\n",
    "            print(f'  data size: {len(self.input_data)}')\n",
    "            print(f'  augmentation factor: {self.augmentation_factor}')\n",
    "            print(f'  early stopping: {self.early_stopping_patience}')\n",
    "            print(f'  epochs (max): {self.max_epochs}')\n",
    "        print(f'  optimizer: {self.optimizer_name}')\n",
    "        print(f'  learning rate: {self.learning_rate}')\n",
    "        print(f'  loss function: {self.loss_function_name}')\n",
    "        print(f'  random seed: {self.random_state}')\n",
    "        print(f'  epochs trained: {self.epochs}')\n",
    "        print(f'L2 norms:')\n",
    "        print(f'  positive: {self.euclidean_dist_positive_class}')\n",
    "        print(f'  negative: {self.euclidean_dist_negative_class}')\n",
    "        print(f'  delta: {self.euclidean_dist_delta}')\n",
    "        print(f'  ratio: {self.euclidean_dist_ratio}')\n",
    "        print(f' ')\n",
    "\n",
    "        if show_history:\n",
    "            plot_training_loss(self.fit_history)\n",
    "\n",
    "    def plot_output_data(self, plot_type='line'): \n",
    "        #!!! MAKE SURE DATA_COLS IS CORRECT AND CORRESPONDS TO THE DATA PASSED TO THE AUTOENCODER !!!\n",
    "        #plot_type must be one of ['datapoints', 'violin', 'overlap', 'line']\n",
    "        \n",
    "        positive_class_input_dataframe = create_dataframe(self.input_data, 'positive class (input)', self.data_cols)\n",
    "        positive_class_output_dataframe = create_dataframe(self.autoencoder_data, 'positive class (output)', self.data_cols)\n",
    "        negative_class_output_dataframe = create_dataframe(self.random_dissimilar_data, 'negative class (output)', self.data_cols)\n",
    "        \n",
    "        combined_dataframe = pd.concat([positive_class_input_dataframe, positive_class_output_dataframe, negative_class_output_dataframe], ignore_index=True)\n",
    "        password_ascii = [ord(c) for c in self.password]\n",
    "        for idx, key_value in enumerate(password_ascii):\n",
    "            combined_dataframe['key'+str(idx)] = key_value\n",
    "\n",
    "        d_regex = re.compile(DURATION_COL_PATTERN)\n",
    "        pp_regex = re.compile(PP_COL_PATTERN)\n",
    "        pr_regex = re.compile(PR_COL_PATTERN)\n",
    "        rp_regex = re.compile(RP_COL_PATTERN)\n",
    "        rr_regex = re.compile(RR_COL_PATTERN)        \n",
    "        \n",
    "        if any((match := d_regex.match(column)) for column in self.data_cols):\n",
    "            plot_d_data(combined_dataframe, self.password, target_category=None, plot_type=plot_type,\n",
    "                        trim_outliers=True, display=True, save=False)\n",
    "        \n",
    "        if any((match := pp_regex.match(column)) for column in self.data_cols):\n",
    "            plot_pp_data(combined_dataframe, self.password, target_category=None, plot_type=plot_type,\n",
    "                         trim_outliers=True, display=True, save=False)\n",
    "            \n",
    "        if any((match := pr_regex.match(column)) for column in self.data_cols):\n",
    "            plot_pr_data(combined_dataframe, self.password, target_category=None, plot_type=plot_type,\n",
    "                         trim_outliers=True, display=True, save=False)\n",
    "\n",
    "        if any((match := rp_regex.match(column)) for column in self.data_cols):\n",
    "            plot_rp_data(combined_dataframe, self.password, target_category=None, plot_type=plot_type, \n",
    "                         trim_outliers=True, display=True, save=False)\n",
    "\n",
    "        if any((match := rr_regex.match(column)) for column in self.data_cols):\n",
    "            plot_rr_data(combined_dataframe, self.password, target_category=None, plot_type=plot_type, \n",
    "                         trim_outliers=True, display=True, save=False)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bffcfb07-9eed-447c-9f71-b366bea0c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@background # concurrency breaks reproducability!\n",
    "def train_autoencoder(autoencoder, index):\n",
    "    start = time.time()    \n",
    "    print(f'Starting initialization on autoencoder #{index}.')\n",
    "    autoencoder.initialize()\n",
    "    print(f'Finished training autoencoder #{index} after {(time.time() - start):.2f} seconds.')\n",
    "\n",
    "    return {**autoencoder.output['meta-info'], **autoencoder.output['euclidean distance evaluation']}\n",
    "\n",
    "def cycle_eval_autoencoders(X, password, optimizers, loss_functions, learning_rates, random_states_count=20, augmentation_factor=10, output=None):        \n",
    "    autoencoders = []\n",
    "    autoencoder_results = []\n",
    "\n",
    "    optimizers_count = len(optimizers)\n",
    "    loss_functions_count = len(loss_functions)\n",
    "    learning_rates_count = len(learning_rates)\n",
    "\n",
    "    total_training_cycles = optimizers_count * loss_functions_count * learning_rates_count * random_states_count\n",
    "    \n",
    "    print(f'Starting training & evaluation for {optimizers_count*loss_functions_count*learning_rates_count*random_states_count} autoencoders:')\n",
    "    \n",
    "    for opt_idx in range(optimizers_count):       \n",
    "        optimizer_cycles = opt_idx * loss_functions_count * learning_rates_count * random_states_count\n",
    "        \n",
    "        for loss_idx in range(loss_functions_count):           \n",
    "            loss_function_cycles = loss_idx * learning_rates_count * random_states_count\n",
    "            \n",
    "            for learn_idx in range(learning_rates_count):\n",
    "                learning_rate_cycles = learn_idx * random_states_count\n",
    "\n",
    "                for rand_idx in range(random_states_count):       \n",
    "                    random_seed = random.randrange(1000000000)\n",
    "                    current_cycle = 1+rand_idx+learning_rate_cycles+loss_function_cycles+optimizer_cycles\n",
    "\n",
    "                    autoencoder = Autoencoder(X, password, \n",
    "                                              optimizers[opt_idx], \n",
    "                                              learning_rates[learn_idx], \n",
    "                                              loss_functions[loss_idx], \n",
    "                                              epochs=Autoencoder.DEFAULT_EPOCHS,\n",
    "                                              early_stopping_patience=20,\n",
    "                                              random_state=random_seed, \n",
    "                                              init=False)\n",
    "\n",
    "                    if output != None:\n",
    "                        autoencoder_evaluation_row_to_csv(train_autoencoder(autoencoder, current_cycle), output[0], output[1])\n",
    "                        #autoencoder.print_evaluation(show_history=False)\n",
    "                        del autoencoder\n",
    "                        gc.collect()\n",
    "                    else:\n",
    "                        autoencoder_results.append(train_autoencoder(autoencoder, current_cycle))\n",
    "                        autoencoders.append(autoencoder)\n",
    "    \n",
    "    if output == None:\n",
    "        return autoencoder_results, autoencoders\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def autoencoder_evaluation_row_to_csv(evaluation, filename, path):    \n",
    "    full_filename = f'{path}/{filename}.csv'\n",
    "    if os.path.exists(full_filename) == False:\n",
    "        with open(full_filename, 'w', newline='') as file:\n",
    "            csv.DictWriter(file, delimiter=\";\", fieldnames=evaluation.keys()).writeheader()\n",
    "     \n",
    "    with open(full_filename, 'a', newline='') as file:\n",
    "        csv.DictWriter(file, delimiter=\";\", fieldnames=evaluation.keys()).writerow(evaluation)\n",
    "        \n",
    "def autoencoder_evaluation_to_csv(evaluations, filename, path):\n",
    "    dataframe = pd.DataFrame.from_dict(evaluations)   \n",
    "    timestamp = str(datetime.datetime.today()).replace(' ', '_').replace(':', '-').split(\".\")[0]\n",
    "    \n",
    "    dataframe.to_csv(f'{path}/{filename}__{timestamp}.csv', sep=';', decimal=',', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7f101b6-eceb-4d1b-85b9-417a438b3d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced optimizers and loss functions through previous cyclic evaluations\n",
    "def find_autoencoder_hyperparameters(data, password, direct_to_file=False):\n",
    "    optimizers = [\n",
    "        'Adagrad',\n",
    "        'RMSprop',\n",
    "        \n",
    "        #'Adam', \n",
    "        #'Lion',\n",
    "        #'AdamW', \n",
    "        #'Adadelta', \n",
    "        #'Adamax', \n",
    "        #'Ftrl', \n",
    "        #'Nadam',\n",
    "        #'SGD', \n",
    "    ]\n",
    "    \n",
    "    loss_functions = [\n",
    "        'Hinge', # best performing for autoencoders through trial and error\n",
    "        \n",
    "        #'MeanSquaredError', \n",
    "        #'MeanSquaredLogarithmicError',\n",
    "        #'MeanAbsoluteError',\n",
    "        #'MeanAbsolutePercentageError',\n",
    "        #'CosineSimilarity',\n",
    "        #'Huber',\n",
    "        #'Poisson',\n",
    "    ]\n",
    "\n",
    "    EVALUATIONS_OUTPUT_PATH = \"evaluations\"\n",
    "    if not os.path.exists(EVALUATIONS_OUTPUT_PATH):\n",
    "        os.makedirs(EVALUATIONS_OUTPUT_PATH)\n",
    "\n",
    "    filename = 'autoencoder_evaluations'\n",
    "    direct_output = (filename, EVALUATIONS_OUTPUT_PATH) if direct_to_file else None\n",
    "\n",
    "    # 0.1 and 0.005 were not suited well in previous attempts\n",
    "    #[0.05, 0.03, 0.01] \n",
    "    # 0.03 seems to perform the best for Adagrad with Hinge Loss\n",
    "    learning_rates = [0.03]\n",
    "    evaluation_results, autoencoders = cycle_eval_autoencoders(data, password, \n",
    "                                                               optimizers, loss_functions, learning_rates, \n",
    "                                                               random_states_count=50,\n",
    "                                                               output=direct_output)\n",
    "        \n",
    "    return evaluation_results, autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c505166-3656-4229-add7-d94c8b5ab3bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evals, autoencoders = find_autoencoder_hyperparameters(positive_class_data[DATA].values, DATA_PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293ea95c-c849-4bd7-ad93-5b6caeff9f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
